{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machine & PCA with Credit Card Default Data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOP5Z/bbJby+B4LRTEWLUOB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishwor2048/Machine-Learning/blob/main/Support_Vector_Machine_%26_PCA_with_Credit_Card_Default_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TexcXN2qRa9n"
      },
      "source": [
        "# **Support Vector Machine on Credit Card Default Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAMbi36nRAqF"
      },
      "source": [
        "dataset: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7jat2cQxnM"
      },
      "source": [
        "# Importing all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "from sklearn.utils import resample # Downsampling the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale # Scale and center the data\n",
        "from sklearn.svm import SVC # This will bring support vector machine for classification\n",
        "from sklearn.model_selection import GridSearchCV # This one will do the cross validation part\n",
        "from sklearn.metrics import confusion_matrix # This create a confusion matrix\n",
        "from sklearn.metrics import plot_confusion_matrix # Draws / plots a confusion matrix\n",
        "from sklearn.decomposition import PCA # to perform PCA to plot the data\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJaIhYEYScMi"
      },
      "source": [
        "# Time to import the data (credit card default data, from UCLA Machine Learning repository data which provides complete data for good machine learnning model which helps us to learn, build and make the machine learning model and use for our learning and organizational purpose.\n",
        "df = pd.read_csv(\"default_of_credit_card_clients.csv\", header=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSoL0GRvTwdP"
      },
      "source": [
        "# Let's look the first 5 rows of the dataset to briefly understand what data actually looks like and how we should analyze the data while moving forward\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxFaquULUtjI"
      },
      "source": [
        "# Renaming the target variable\n",
        "df.rename({'default payment next month': 'DEFAULT'}, axis='columns', inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyE1CNYZZYeY"
      },
      "source": [
        "# Dropping the unnecessary items\n",
        "df.drop('ID', axis=1, inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsJLXoSbZqf-"
      },
      "source": [
        "# Let's check what datatypes we have in the dataset before we start with implementing the missing values\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6zQNm0eZ94c"
      },
      "source": [
        "# Here is what data says from UCLA Repositor:\n",
        "\"\"\"\"\n",
        "Data Set Information:\n",
        "\n",
        "This research aimed at the case of customersâ€™ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel â€œSorting Smoothing Methodâ€ to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.\n",
        "\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:\n",
        "X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
        "X2: Gender (1 = male; 2 = female).\n",
        "X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "X4: Marital status (1 = married; 2 = single; 3 = others).\n",
        "X5: Age (year).\n",
        "X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
        "X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.\n",
        "X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk0oE_sfbQAq"
      },
      "source": [
        "# Let's have a sanity check if SEX has only 1 & 2 (1 for male and 2 for female)\n",
        "df[\"SEX\"].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A03MvqRVbc3a"
      },
      "source": [
        "# Checking for education if Education (1 = graduate school; 2 = university; 3 = high school; 4 = others)\n",
        "df[\"EDUCATION\"].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3pYoThLbs8z"
      },
      "source": [
        "# Let's check the marriage column\n",
        "df['MARRIAGE'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db_Az_Rgb_fO"
      },
      "source": [
        "# Dealing with missing values in the dataset, at this point we are considering missing value to zero since we don't have a lot of information about the dataset since the information\n",
        "# is paid, at this moment, just for the shake of learning we will consider missng values to 0, but in the future we can work out on understanding what was actual missing value\n",
        "# and what was 0 representing for.\n",
        "len(df.loc[(df['EDUCATION'] == 0) | (df['MARRIAGE'] == 0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp_fNgOocx-W"
      },
      "source": [
        "# Let's check the total length of the dataset\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bzfM9jbdT_P"
      },
      "source": [
        "# Removing 0 valued rows (missing values) from Education and Marriage columns\n",
        "df_no_missing = df.loc[(df['EDUCATION'] != 0) & (df['MARRIAGE'] != 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKXygMDwfNtm"
      },
      "source": [
        "df_no_missing.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-mcJTJdfO_C"
      },
      "source": [
        "len(df_no_missing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r-I4H5sfVUG"
      },
      "source": [
        "# Now verifying if Education and Marriage columns still have zero values (missing values)\n",
        "df_no_missing['EDUCATION'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw6fmIpffhWZ"
      },
      "source": [
        "df_no_missing['MARRIAGE'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsdU2kZvfkK1"
      },
      "source": [
        "# Downsampling the dataset\n",
        "# We will take 5000 each for defaulted and not defaulted data from the dataset since Support Vector Machine works very well on those kind of dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-AL4CdXfqvK"
      },
      "source": [
        "len(df_no_missing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNl42QxSfulg"
      },
      "source": [
        "# Seperating dataset into defaulted and not defaulted data\n",
        "df_no_default = df_no_missing[df_no_missing['DEFAULT'] == 0]\n",
        "df_default = df_no_missing[df_no_missing['DEFAULT'] == 1]\n",
        "\n",
        "print(len(df_default))\n",
        "print(len(df_no_default))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMMpQdK7gKec"
      },
      "source": [
        "# Time to downsample for 1000 rows each for defaulted and not defaulted dataset\n",
        "df_no_default_downsampled = resample(df_no_default, replace=False, n_samples = 1000, random_state = 42)\n",
        "len(df_no_default_downsampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj7L_-Mjg0hQ"
      },
      "source": [
        "df_default_downsampled = resample(df_default, replace=False, n_samples = 1000, random_state = 42)\n",
        "len(df_default_downsampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MNk80Iig6wD"
      },
      "source": [
        "# Time to merge default and not default 1000 each samples\n",
        "df_downsample = pd.concat([df_no_default_downsampled, df_default_downsampled])\n",
        "len(df_downsample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SrKLop2hZac"
      },
      "source": [
        "# Time to format the data for support vector machine\n",
        "\n",
        "# Setting up X as independent Variables (Predictors)\n",
        "X = df_downsample.drop(\"DEFAULT\", axis=1).copy()\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy9idWEEhvNZ"
      },
      "source": [
        "# Setting up the predictive variable (y) (Dependent on X variable)\n",
        "y = df_downsample['DEFAULT'].copy()\n",
        "y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa_s2gi3h1fo"
      },
      "source": [
        "# One-Hot Encoding for the categorical variable\n",
        "# Categorical variables (Sex, Education, Marriage and Pay)\n",
        "X_encoded = pd.get_dummies(X, columns=['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'])\n",
        "X_encoded.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40OgU9kljK3T"
      },
      "source": [
        "# Centering and scaling the data since the feature values are not similar (same scale) in the original dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, random_state = 42, test_size = 0.2)\n",
        "X_train_scaled = scale(X_train)\n",
        "X_test_scaled = scale(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6VXKpYWjo36"
      },
      "source": [
        "# Building the preliminary Support Vector Machine Model\n",
        "clf_svm = SVC(random_state=42)\n",
        "clf_svm.fit(X_train_scaled, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCngwyIhkqYN"
      },
      "source": [
        "# Time to build confusion matrix with the actual and predicted results\n",
        "plot_confusion_matrix(clf_svm, X_test_scaled, y_test, values_format='d', display_labels=[\"Did Not Default\", \"Defaulted\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzOEUBAulCgR"
      },
      "source": [
        "# Since this time model did not do great job, let's use the cross validation to optimize the parameters\n",
        "# We will use Optimization Parameters with Cross Validation and GridSearchCV()\n",
        "param_grid = [\n",
        "              {\n",
        "                'C': [0.5, 1, 10, 100], # C is the regularization parameter which got to be greater than zero\n",
        "               'gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],\n",
        "               'kernel': ['rbf']\n",
        "              }\n",
        "]\n",
        "\n",
        "optimal_params = GridSearchCV(\n",
        "    SVC(),\n",
        "    param_grid,\n",
        "    cv = 5,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "optimal_params.fit(X_train_scaled, y_train)\n",
        "print(optimal_params.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5XXWa-0m0Q_"
      },
      "source": [
        "# Building support vector machine based on the optimal parameters\n",
        "clf_svm = SVC(random_state = 42, C = 1, gamma = 0.01, kernel='rbf')\n",
        "clf_svm.fit(X_train_scaled, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT_BrqPJoYQu"
      },
      "source": [
        "# Let's plot the confusion matrix with the new results\n",
        "plot_confusion_matrix(clf_svm, X_test_scaled, y_test, values_format='d', display_labels = ['Did Not Default', \"Defaulted\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcBy6xWdov4N"
      },
      "source": [
        "# Time to plot the decision boundary\n",
        "len(df_downsample.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z03RWtc8o5Q_"
      },
      "source": [
        "# Since we have large number of columns preventing us to plot that big size of the dimensions, we need to use the PCA to collapse the number of columns to just two number of columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsLdBmtRpD75"
      },
      "source": [
        "pca = PCA()\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
        "labels = [str(x) for x in range(1, len(per_var)+1)]\n",
        "\n",
        "plt.bar(x=range(1, len(per_var) + 1), height = per_var)\n",
        "plt.tick_params(\n",
        "    axis = 'x',\n",
        "    which = 'both',\n",
        "    bottom = False,\n",
        "    top = False,\n",
        "    labelbottom = False\n",
        ")\n",
        "plt.ylabel('Percentage of Explained Variance')\n",
        "plt.xlabel('Principle Components')\n",
        "plt.title('Scree Plot')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf9kbl_wqQYw"
      },
      "source": [
        "# Training Hyperparameters on different values to get even better results, with the output of PCA\n",
        "train_pc1_coords = X_train_pca[:, 0]\n",
        "train_pc2_coords = X_train_pca[:, 1]\n",
        "\n",
        "# pc1 contains x-axis coordinates and pc2 contains y-axis coordinates after performing the PCA on the original data\n",
        "\n",
        "pca_train_scaled = scale(np.column_stack((train_pc1_coords, train_pc2_coords)))\n",
        "\n",
        "# time to optimize the svm parameters to fit the x and y-axis coordinates\n",
        "param_grid = [\n",
        "              {'C': [1, 10, 100, 1000],\n",
        "               'gamma': ['scale', 1, 0.1, 0.001, 0.0001],\n",
        "               'kernel': ['rbf']}\n",
        "]\n",
        "\n",
        "optional_params = GridSearchCV(\n",
        "    SVC(),\n",
        "    param_grid,\n",
        "    cv = 5,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "optimal_params.fit(pca_train_scaled, y_train)\n",
        "print(optimal_params.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VBnMr77rfyL"
      },
      "source": [
        "clf_svm = SVC(random_state = 42, C = 100, gamma = 0.01, kernel='rbf')\n",
        "clf_svm.fit(pca_train_scaled, y_train)\n",
        "\n",
        "# Transform the test dataset with the PCA\n",
        "X_test_pca = pca.transform(X_train_scaled)\n",
        "\n",
        "test_pc1_coords = X_test_pca[:, 0]\n",
        "test_pc2_coords = X_test_pca[:, 1]\n",
        "\n",
        "x_min = test_pc1_coords.min() - 1\n",
        "x_max = test_pc1_coords.max() + 1\n",
        "\n",
        "y_min = test_pc2_coords.min() - 1\n",
        "y_max = test_pc2_coords.max() + 1\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(start = x_min, stop = x_max, step = 0.1),\n",
        "                     np.arange(start = y_min, stop = y_max, step = 0.01))\n",
        "\n",
        "Z = clf_svm.predict(np.column_stack((xx.ravel(), yy.ravel())))\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10, 10))\n",
        "ax.contour(xx, yy, Z, alpha = 0.1)\n",
        "\n",
        "cmap = colors.ListedColormap(['#e41a1c', '#4daf4a'])\n",
        "\n",
        "scatter = ax.scatter(test_pc1_coords, test_pc2_coords, c = y_train, cmap = cmap, s = 100, edgecolors = 'k', alpha = 0.8)\n",
        "legend = ax.legend(scatter.legend_elements()[0], \n",
        "                   scatter.legend_elements()[1],\n",
        "                   loc = 'upper right')\n",
        "legend.get_texts()[0].set_text(\"No Default\")\n",
        "legend.get_texts()[1].set_text(\"Yes Default\")\n",
        "\n",
        "ax.set_ylabel(\"PC2\")\n",
        "ax.set_xlabel(\"PC1\")\n",
        "ax.set_title(\"Decision Surface using the PCA transformed / Projected features\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}